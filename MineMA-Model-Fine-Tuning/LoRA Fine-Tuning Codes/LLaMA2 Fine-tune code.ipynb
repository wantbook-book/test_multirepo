{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac949-4150-4091-b0c3-2968ab5e385c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入环境\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "import transformers\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098d9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入数据集，分为训练数据集和测试数据集\n",
    "df = pd.read_json('')\n",
    "train_df_three = df.sample(frac=0.95, random_state=123)\n",
    "test_df_three = df.drop(train_df_three.index)\n",
    "ds = Dataset.from_pandas(train_df_three)\n",
    "ds_test = Dataset.from_pandas(test_df_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入原始模型\n",
    "tokenizer = AutoTokenizer.from_pretrained('', use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained('', device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503a5fa-9621-4495-9035-8e7ef6525691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 1024\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<s>[INST] <<SYS>>\\nYou are a Large Language Model, and your task is to answer questions posed by users about Minecraft. Utilize your knowledge and understanding of the game to provide detailed, accurate, and helpful responses. Use your capabilities to assist users in solving problems, understanding game mechanics, and enhancing their Minecraft experience.\\n<</SYS>>\\n\\n{example['instruction']} [/INST]\", add_special_tokens=False) \n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1] \n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH: \n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f870d6-73a9-4b0f-8abf-687b32224ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id_test = ds_test.map(process_func, remove_columns=ds_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LoRA参数设置\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,\n",
    "    r=32, \n",
    "    lora_alpha=64, \n",
    "    lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2489c5-eaab-4e1f-b06a-c3f914b4bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#设定训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=False,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\"\n",
    ")\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    eval_dataset=tokenized_id_test,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9bc36-b297-45af-99e1-d4c4d82be081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "save_path=\"\"\n",
    "trainer.model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cf6e1-eb5c-4378-b5ed-a2095b44db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, \n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "new_model_path = \"\"\n",
    "lora_model_path = \"\"\n",
    "base_model_path = \"\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path, config=config)\n",
    "model = model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "\n",
    "model.save_pretrained(new_model_path)\n",
    "tokenizer.save_pretrained(new_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
